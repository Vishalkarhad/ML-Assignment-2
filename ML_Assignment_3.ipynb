{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**1. What is regression analysis?**\n",
        "\n",
        "Regression analysis is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. It's often used for prediction and forecasting.\n",
        "\n",
        "**2. Explain the difference between linear and nonlinear regression.**\n",
        "\n",
        "* **Linear Regression:** Assumes a linear relationship between the dependent and independent variables. The model can be represented by a straight line.\n",
        "* **Nonlinear Regression:** Allows for more complex relationships, such as polynomial or exponential. The model is not a straight line.\n",
        "\n",
        "**3. What is the difference between simple linear regression and multiple linear regression?**\n",
        "\n",
        "* **Simple Linear Regression:** Involves one independent variable and one dependent variable.\n",
        "* **Multiple Linear Regression:** Involves multiple independent variables and one dependent variable.\n",
        "\n",
        "**4. How is the performance of a regression model typically evaluated?**\n",
        "\n",
        "Common metrics for evaluating regression model performance include:\n",
        "\n",
        "* **Mean Squared Error (MSE):** Measures the average squared difference between predicted and actual values.\n",
        "* **Root Mean Squared Error (RMSE):** The square root of MSE, providing a more interpretable measure.\n",
        "* **R-squared:** Indicates the proportion of variance in the dependent variable explained by the independent variables.\n",
        "\n",
        "**5. What is overfitting in the context of regression models?**\n",
        "\n",
        "Overfitting occurs when a model is too complex and fits the training data too closely, leading to poor performance on new, unseen data. This can be caused by having too many features or a model that is too flexible.\n",
        "\n",
        "\n",
        "\n",
        "**6. What is logistic regression used for?**\n",
        "\n",
        "Logistic regression is a statistical model used for predicting binary outcomes (e.g., yes/no, true/false). It's often used in classification problems.\n",
        "\n",
        "**7. How does logistic regression differ from linear regression?**\n",
        "\n",
        "* **Output:** Linear regression predicts a continuous value, while logistic regression predicts a probability between 0 and 1.\n",
        "* **Model:** Logistic regression uses a sigmoid function to transform the linear combination of features into a probability.\n",
        "\n",
        "**8. Explain the concept of odds ratio in logistic regression.**\n",
        "\n",
        "The odds ratio in logistic regression represents the change in the odds of an event occurring for a unit increase in the independent variable.\n",
        "\n",
        "**9. What is the sigmoid function in logistic regression?**\n",
        "\n",
        "The sigmoid function maps any real value to a value between 0 and 1, making it suitable for predicting probabilities.\n",
        "\n",
        "**10. How is the performance of a logistic regression model evaluated?**\n",
        "\n",
        "Common metrics for evaluating logistic regression model performance include:\n",
        "\n",
        "* **Accuracy:** The proportion of correct predictions.\n",
        "* **Precision:** The proportion of positive predictions that are actually positive.\n",
        "* **Recall:** The proportion of actual positive cases that are correctly predicted as positive.\n",
        "* **F1-score:** A harmonic mean of precision and recall.\n",
        "\n",
        "\n",
        "\n",
        "**11. What is a decision tree?**\n",
        "\n",
        "A decision tree is a supervised machine learning algorithm that can be used for both classification and regression tasks. It creates a tree-like model with nodes representing decisions and branches representing possible outcomes.\n",
        "\n",
        "**12. How does a decision tree make predictions?**\n",
        "\n",
        "To make a prediction, a decision tree starts at the root node and follows the branches based on the values of the features until it reaches a leaf node, which contains the prediction.\n",
        "\n",
        "**13. What is entropy in the context of decision trees?**\n",
        "\n",
        "Entropy is a measure of the impurity or disorder in a dataset. It's used to determine the best feature to split on at each node of the decision tree.\n",
        "\n",
        "**14. What is pruning in decision trees?**\n",
        "\n",
        "Pruning is a technique used to reduce the size and complexity of a decision tree to prevent overfitting. It involves removing branches or nodes that don't contribute significantly to the model's performance.\n",
        "\n",
        "**15. How do decision trees handle missing values?**\n",
        "\n",
        "Decision trees can handle missing values by:\n",
        "\n",
        "* **Replacing missing values:** Using the mean, median, or mode for numerical features, or the most frequent category for categorical features.\n",
        "* **Creating a separate branch:** Creating a separate branch for missing values.\n",
        "\n",
        "\n",
        "\n",
        "**16. What is a support vector machine (SVM)?**\n",
        "\n",
        "SVMs are a class of supervised learning algorithms that can be used for both classification and regression. They find the optimal hyperplane that separates data points into different classes.\n",
        "\n",
        "**17. Explain the concept of margin in SVM.**\n",
        "\n",
        "The margin is the distance between the hyperplane and the nearest data points (support vectors). SVMs aim to maximize the margin to improve generalization.\n",
        "\n",
        "**18. What are support vectors in SVM?**\n",
        "\n",
        "Support vectors are the data points that lie closest to the hyperplane. They are crucial in determining the position and orientation of the hyperplane.\n",
        "\n",
        "**19. How does SVM handle non-linearly separable data?**\n",
        "\n",
        "SVMs can handle non-linearly separable data by using **kernels**. Kernels map the data into a higher-dimensional feature space where it might be linearly separable.\n",
        "\n",
        "**20. What are the advantages of SVM over other classification algorithms?**\n",
        "\n",
        "* **Robust to outliers:** SVMs are not sensitive to outliers.\n",
        "* **Effective for high-dimensional data:** SVMs can handle large numbers of features.\n",
        "* **Good generalization performance:** SVMs often perform well on unseen data.\n",
        "\n",
        "\n",
        "\n",
        "**21. What is the Naive Bayes algorithm?**\n",
        "\n",
        "Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem. It assumes that features are independent given the class label.\n",
        "\n",
        "**22. Why is it called \"Naive Bayes\"?**\n",
        "\n",
        "It's called \"Naive Bayes\" because it makes the simplifying assumption of feature independence, which is often not strictly true in real-world data.\n",
        "\n",
        "**23. How does Naive Bayes handle continuous and categorical features?**\n",
        "\n",
        "* **Continuous features:** Naive Bayes typically assumes a Gaussian distribution for continuous features.\n",
        "* **Categorical features:** The probability of each category is estimated based on the frequency of occurrence in the training data.\n",
        "\n",
        "**24. Explain the concept of prior and posterior probabilities in Naïve Bayes.**\n",
        "\n",
        "* **Prior probability:** The probability of a class label before observing any features.\n",
        "* **Posterior probability:** The probability of a class label given the observed features.\n",
        "\n",
        "**25. What is Laplace smoothing and why is it used in Naïve Bayes?**\n",
        "\n",
        "Laplace smoothing is a technique used to avoid zero probabilities in the case of unseen categories. It adds a small constant to the count of each category.\n",
        "\n",
        "**26. Can Naïve Bayes be used for regression tasks?**\n",
        "\n",
        "Yes, Naive Bayes can be used for regression tasks by using a Gaussian distribution to model the continuous target variable.\n",
        "\n",
        "**27. How do you handle missing values in Naive Bayes?**\n",
        "\n",
        "Missing values can be handled in Naive Bayes by:\n",
        "\n",
        "* **Imputing missing values:** Replacing missing values with estimated values.\n",
        "* **Ignoring missing values:** Excluding instances with missing values from the analysis.\n",
        "\n",
        "**28. What are some common applications of Naive Bayes?**\n",
        "\n",
        "* **Text classification:** Classifying documents based on their content.\n",
        "* **Spam filtering:** Identifying spam emails.\n",
        "* **Recommendation systems:** Suggesting items based on user preferences.\n",
        "\n",
        "**29. Explain the concept of feature independence assumption in Naïve Bayes.**\n",
        "\n",
        "The Naive Bayes algorithm assumes that features are independent given the class label. This assumption simplifies the calculations but may not always hold true in real-world data.\n"
      ],
      "metadata": {
        "id": "AB2JM2v3DSjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**30. How does Naive Bayes handle categorical features with a large number of categories?**\n",
        "\n",
        "Naive Bayes handles categorical features with a large number of categories by estimating the probability of each category directly from the training data. This can be computationally efficient, even with many categories. However, if a category has a very low frequency, it may lead to unstable estimates.\n",
        "\n",
        "\n",
        "\n",
        "**31. What is the curse of dimensionality, and how does it affect machine learning algorithms?**\n",
        "\n",
        "The curse of dimensionality refers to the challenges that arise when dealing with high-dimensional data. As the number of dimensions increases, the data points become more sparse, making it difficult for algorithms to learn meaningful patterns. This can lead to overfitting, increased computational complexity, and decreased performance.\n",
        "\n",
        "\n",
        "**32. Explain the bias-variance tradeoff and its implications for machine learning models.**\n",
        "\n",
        "The bias-variance tradeoff is a fundamental concept in machine learning that describes the balance between underfitting and overfitting. A model with high bias is underfit and cannot capture the underlying patterns in the data. A model with high variance is overfit and fits the training data too closely, leading to poor generalization. The goal is to find a balance between these two extremes.\n",
        "\n",
        "\n",
        "\n",
        "**33. What is cross-validation, and why is it used?**\n",
        "\n",
        "Cross-validation is a technique used to evaluate the performance of a machine learning model on unseen data. It involves dividing the data into multiple folds, training the model on some folds and testing it on the remaining folds. This helps to prevent overfitting and provides a more reliable estimate of the model's performance.\n",
        "\n",
        "\n",
        "\n",
        "**34. Explain the difference between parametric and non-parametric machine learning algorithms.**\n",
        "\n",
        "* **Parametric algorithms:** Make assumptions about the underlying data distribution and fit a model based on these assumptions. Examples include linear regression and logistic regression.\n",
        "* **Non-parametric algorithms:** Do not make assumptions about the data distribution and learn the model directly from the data. Examples include decision trees, support vector machines, and k-nearest neighbors.\n",
        "\n",
        "\n",
        "\n",
        "**35. What is feature scaling, and why is it important in machine learning?**\n",
        "\n",
        "Feature scaling is the process of transforming numerical features to a common scale to improve model performance and prevent bias. It's important because algorithms like gradient descent can be sensitive to the scale of features.\n",
        "\n",
        "\n",
        "\n",
        "**36. What is regularization, and why is it used in machine learning?**\n",
        "\n",
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. This penalty term discourages the model from fitting the training data too closely. Common regularization techniques include L1 and L2 regularization.\n",
        "\n",
        "\n",
        "\n",
        "**37. Explain the concept of ensemble learning and give an example.**\n",
        "\n",
        "Ensemble learning combines multiple models to improve performance. Examples include random forests, which combine multiple decision trees, and gradient boosting, which combines multiple weak learners.\n",
        "\n",
        "\n",
        "\n",
        "**38. What is the difference between bagging and boosting?**\n",
        "\n",
        "* **Bagging:** Creates multiple models using bootstrap samples of the training data and averages their predictions.\n",
        "* **Boosting:** Creates multiple models sequentially, with each model focusing on correcting the errors of the previous models.\n",
        "\n",
        "\n",
        "\n",
        "**39. What is the difference between a generative model and a discriminative model?**\n",
        "\n",
        "* **Generative models:** Learn the joint probability distribution of the features and the target variable.\n",
        "* **Discriminative models:** Learn the conditional probability of the target variable given the features.\n",
        "\n",
        "\n",
        "\n",
        "**40. Explain the concept of batch gradient descent and stochastic gradient descent.**\n",
        "\n",
        "* **Batch gradient descent:** Calculates the gradient of the loss function using the entire training set.\n",
        "* **Stochastic gradient descent:** Calculates the gradient using a single random data point.\n",
        "\n",
        "\n",
        "\n",
        "**41. What is the K-nearest neighbors (KNN) algorithm, and how does it work?**\n",
        "\n",
        "KNN is a non-parametric algorithm that classifies or regresses new data points based on the majority class or average values of the k nearest neighbors in the training set.\n",
        "\n",
        "**42. What are the disadvantages of the K-nearest neighbors algorithm?**\n",
        "\n",
        "* Can be computationally expensive for large datasets.\n",
        "* Sensitive to the choice of k.\n",
        "* Can be sensitive to the scale of features.\n",
        "\n",
        "\n",
        "\n",
        "**43. Explain the concept of one-hot encoding and its use in machine learning.**\n",
        "\n",
        "One-hot encoding is a technique used to represent categorical data as binary vectors. For each category, a new binary feature is created. This is useful for machine learning algorithms that require numerical input.\n",
        "\n",
        "\n",
        "\n",
        "**44. What is feature selection, and why is it important in machine learning?**\n",
        "\n",
        "Feature selection is the process of selecting the most relevant features from a dataset to improve model performance and reduce computational cost. It can help to prevent overfitting and make the model more interpretable.\n",
        "\n",
        "\n",
        "\n",
        "**45. Explain the concept of cross-entropy loss and its use in classification tasks.**\n",
        "\n",
        "Cross-entropy loss is a commonly used loss function for classification tasks. It measures the difference between the predicted probability distribution and the true distribution.\n",
        "\n",
        "\n",
        "\n",
        "**46. What is the difference between batch learning and online learning?**\n",
        "\n",
        "* **Batch learning:** Trains the model on the entire training set at once.\n",
        "* **Online learning:** Trains the model on one data point at a time, allowing for updates as new data becomes available.\n",
        "\n",
        "\n",
        "\n",
        "**47. Explain the concept of grid search and its use in hyperparameter tuning.**\n",
        "\n",
        "Grid search is a hyperparameter tuning technique that involves trying all combinations of hyperparameters within a specified grid. It's a brute-force approach that can be computationally expensive for large grids.\n",
        "\n",
        "\n",
        "\n",
        "**48. What are the advantages and disadvantages of decision trees?**\n",
        "\n",
        "* **Advantages:** Easy to interpret, can handle both numerical and categorical features, robust to outliers.\n",
        "* **Disadvantages:** Can be prone to overfitting, sensitive to small changes in the data.\n",
        "\n",
        "\n",
        "\n",
        "**49. What is the difference between L1 and L2 regularization?**\n",
        "\n",
        "* **L1 regularization:** Encourages sparsity, meaning many features may have zero coefficients, leading to simpler models.\n",
        "* **L2 regularization:** Reduces the magnitude of coefficients, preventing individual features from dominating the model.\n",
        "\n",
        "\n",
        "\n",
        "**50. What are some common preprocessing techniques used in machine learning?**\n",
        "\n",
        "* **Feature scaling:** Normalizing or standardizing features to a common scale.\n",
        "* **Handling missing values:** Imputing missing values or removing rows/columns with missing data.\n",
        "* **Outlier detection and removal:** Identifying and removing outliers.\n",
        "* **Feature engineering:** Creating new features from existing ones.\n",
        "\n",
        "\n",
        "\n",
        "**51. What is the difference between a parametric and non-parametric algorithm? Give examples of each.**\n",
        "\n",
        "* **Parametric algorithms:** Make assumptions about the underlying data distribution and fit a model based on these assumptions. Examples include linear regression and logistic regression.\n",
        "* **Non-parametric algorithms:** Do not make assumptions about the data distribution and learn the model directly from the data. Examples include decision trees, support vector machines, and k-nearest neighbors.\n",
        "\n",
        "\n",
        "\n",
        "**52. Explain the bias-variance tradeoff and how it relates to model complexity.**\n",
        "\n",
        "The bias-variance tradeoff is a fundamental concept in machine learning that describes the balance between underfitting and overfitting. A model with high bias is underfit and cannot capture the underlying patterns in the data. A model with high variance is overfit and fits the training data too closely, leading to poor generalization. The goal is to find a balance between these two extremes. Model complexity plays a role in the bias-variance tradeoff: more complex models tend to have higher variance but lower bias, while simpler models tend to have lower variance but higher bias.\n",
        "\n",
        "\n",
        "\n",
        "**53. What are the advantages and disadvantages of using ensemble methods like random forests?**\n",
        "\n",
        "* **Advantages:** Improved performance, reduced overfitting, better generalization.\n",
        "* **Disadvantages:** Can be computationally expensive, more difficult to interpret.\n",
        "\n",
        "\n",
        "\n",
        "**54. Explain the difference between bagging and boosting.**\n",
        "\n",
        "* **Bagging:** Creates multiple models using bootstrap samples of the training data and averages their predictions.\n",
        "* **Boosting:** Creates multiple models sequentially, with each model focusing on correcting the errors of the previous models.\n",
        "\n",
        "\n",
        "\n",
        "**55. What is the purpose of hyperparameter tuning in machine learning?**\n",
        "\n",
        "Hyperparameter tuning is the process of selecting the best values for the hyperparameters of a machine learning model. This can significantly improve model performance.\n",
        "\n",
        "\n",
        "\n",
        "**56. What is the difference between regularization and feature selection?**\n",
        "\n",
        "* **Regularization:** Reduces the impact of individual features to prevent overfitting.\n",
        "* **Feature selection:** Removes irrelevant or redundant features to improve model performance and interpretability.\n",
        "\n",
        "\n",
        "\n",
        "**57. How does the Lasso (L1) regularization differ from Ridge (L2) regularization?**\n",
        "\n",
        "* **Lasso regularization:** Encourages sparsity, meaning many features may have zero coefficients, leading to simpler models.\n"
      ],
      "metadata": {
        "id": "4pxLhInmDzwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**58. Explain the concept of cross-validation and why it is used.**\n",
        "\n",
        "Cross-validation is a technique used to evaluate the performance of a machine learning model on unseen data. It involves dividing the data into multiple folds, training the model on some folds and testing it on the remaining folds. This helps to prevent overfitting and provides a more reliable estimate of the model's performance.\n",
        "\n",
        "\n",
        "\n",
        "**59. What are some common evaluation metrics used for regression tasks?**\n",
        "\n",
        "* **Mean Squared Error (MSE):** Measures the average squared difference between predicted and actual values.\n",
        "* **Root Mean Squared Error (RMSE):** The square root of MSE, providing a more interpretable measure.\n",
        "* **R-squared:** Indicates the proportion of variance in the dependent variable explained by the independent variables.\n",
        "* **Mean Absolute Error (MAE):** Measures the average absolute difference between predicted and actual values.\n",
        "\n",
        "\n",
        "\n",
        "**60. How does the K-nearest neighbors (KNN) algorithm make predictions?**\n",
        "\n",
        "The KNN algorithm classifies or regresses new data points based on the majority class or average values of the k nearest neighbors in the training set.\n",
        "\n",
        "\n",
        "\n",
        "**61. What is the curse of dimensionality, and how does it affect machine learning algorithms?**\n",
        "\n",
        "The curse of dimensionality refers to the challenges that arise when dealing with high-dimensional data. As the number of dimensions increases, the data points become more sparse, making it difficult for algorithms to learn meaningful patterns. This can lead to overfitting, increased computational complexity, and decreased performance.\n",
        "\n",
        "\n",
        "\n",
        "**62. What is feature scaling, and why is it important in machine learning?**\n",
        "\n",
        "Feature scaling is the process of transforming numerical features to a common scale to improve model performance and prevent bias. It's important because algorithms like gradient descent can be sensitive to the scale of features.\n",
        "\n",
        "\n",
        "\n",
        "**63. How does the Naïve Bayes algorithm handle categorical features?**\n",
        "\n",
        "Naive Bayes handles categorical features by estimating the probability of each category directly from the training data.\n",
        "\n",
        "**64. Explain the concept of prior and posterior probabilities in Naïve Bayes.**\n",
        "\n",
        "* **Prior probability:** The probability of a class label before observing any features.\n",
        "* **Posterior probability:** The probability of a class label given the observed features.\n",
        "\n",
        "**65. What is Laplace smoothing, and why is it used in Naïve Bayes?**\n",
        "\n",
        "Laplace smoothing is a technique used to avoid zero probabilities in the case of unseen categories. It adds a small constant to the count of each category.\n",
        "\n",
        "\n",
        "\n",
        "**66. Can Naïve Bayes handle continuous features?**\n",
        "\n",
        "Yes, Naive Bayes can handle continuous features by assuming a Gaussian distribution for them.\n",
        "\n",
        "\n",
        "\n",
        "**67. What are the assumptions of the Naïve Bayes algorithm?**\n",
        "\n",
        "* **Feature independence:** The Naïve Bayes algorithm assumes that features are independent given the class label.\n",
        "* **Gaussian distribution:** For continuous features, it assumes a Gaussian distribution.\n",
        "\n",
        "\n",
        "\n",
        "**68. How does Naïve Bayes handle missing values?**\n",
        "\n",
        "Missing values can be handled in Naive Bayes by:\n",
        "\n",
        "* **Imputing missing values:** Replacing missing values with estimated values.\n",
        "* **Ignoring missing values:** Excluding instances with missing values from the analysis.\n",
        "\n",
        "\n",
        "\n",
        "**69. What are some common applications of Naïve Bayes?**\n",
        "\n",
        "* **Text classification:** Classifying documents based on their content.\n",
        "* **Spam filtering:** Identifying spam emails.\n",
        "* **Recommendation systems:** Suggesting items based on user preferences.\n",
        "\n",
        "\n",
        "\n",
        "**70. Explain the difference between generative and discriminative models.**\n",
        "\n",
        "* **Generative models:** Learn the joint probability distribution of the features and the target variable.\n",
        "* **Discriminative models:** Learn the conditional probability of the target variable given the features.\n",
        "\n",
        "\n",
        "\n",
        "**71. How does the decision boundary of a Naïve Bayes classifier look like for binary classification tasks?**\n",
        "\n",
        "The decision boundary of a Naïve Bayes classifier is typically linear in the feature space, even if the underlying relationships are non-linear. This is due to the assumption of feature independence.\n",
        "\n",
        "\n",
        "\n",
        "**72. What is the difference between multinomial Naïve Bayes and Gaussian Naïve Bayes?**\n",
        "\n",
        "* **Multinomial Naïve Bayes:** Suitable for categorical features.\n",
        "* **Gaussian Naïve Bayes:** Suitable for continuous features.\n",
        "\n",
        "\n",
        "\n",
        "**73. How does Naïve Bayes handle numerical instability issues?**\n",
        "\n",
        "Numerical instability can occur when probabilities become very small or very large. Laplace smoothing can help to mitigate this issue.\n",
        "\n",
        "\n",
        "\n",
        "**74. What is the Laplacian correction, and when is it used in Naïve Bayes?**\n",
        "\n",
        "Laplacian correction is a technique used to avoid zero probabilities in the case of unseen categories. It adds a small constant to the count of each category.\n",
        "\n",
        "\n",
        "\n",
        "**75. Can Naïve Bayes be used for regression tasks?**\n",
        "\n",
        "Yes, Naive Bayes can be used for regression tasks by using a Gaussian distribution to model the continuous target variable.\n",
        "\n",
        "\n",
        "\n",
        "**76. Explain the concept of conditional independence assumption in Naïve Bayes.**\n",
        "\n",
        "The Naive Bayes algorithm assumes that features are independent given the class label. This assumption simplifies the calculations but may not always hold true in real-world data.\n",
        "\n",
        "\n",
        "\n",
        "**77. How does Naïve Bayes handle categorical features with a large number of categories?**\n",
        "\n",
        "Naive Bayes handles categorical features with a large number of categories by estimating the probability of each category directly from the training data. This can be computationally efficient, even with many categories. However, if a category has a very low frequency, it may lead to unstable estimates.\n",
        "\n",
        "\n",
        "\n",
        "**78. What are some drawbacks of the Naïve Bayes algorithm?**\n",
        "\n",
        "* **Strong assumption of feature independence:** This assumption may not always hold true in real-world data.\n",
        "* **Sensitive to imbalanced datasets:** Naïve Bayes can be biased towards the majority class.\n",
        "* **Limited expressiveness:** Naïve Bayes cannot model complex relationships between features.\n",
        "\n",
        "\n",
        "\n",
        "**79. Explain the concept of smoothing in Naïve Bayes.**\n",
        "\n",
        "Smoothing is a technique used to avoid zero probabilities in the case of unseen categories. Laplace smoothing is a common smoothing technique.\n",
        "\n",
        "\n",
        "\n",
        "**80. How does Naïve Bayes handle imbalanced datasets?**\n",
        "\n",
        "Naïve Bayes can be sensitive to imbalanced datasets, as it may be biased towards the majority class. To address this, techniques like class weighting or oversampling can be used.\n"
      ],
      "metadata": {
        "id": "6b9Xz6b8Etq_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6hHQg5aCrys"
      },
      "outputs": [],
      "source": []
    }
  ]
}